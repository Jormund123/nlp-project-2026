{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a66874",
   "metadata": {},
   "source": [
    "# üöÄ Phase 3: CodeBERT Training on RAW Code (Task B)\n",
    "\n",
    "**Project**: SemEval-2026 Task 13 - Machine-Generated Code Detection  \n",
    "**Phase**: 3 (T015-T024) - CodeBERT Implementation  \n",
    "**Input**: RAW code (preserves AI fingerprints)  \n",
    "\n",
    "## Setup\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** (Colab)\n",
    "2. Upload `task_b_train.parquet` and `task_b_val.parquet` (see EDA stats in repo)\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate sklearn torch joblib pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e691be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"üìÅ Upload task_b_train.parquet and task_b_val.parquet\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# ============== Configuration =============\n",
    "SEED = 42\n",
    "CONFIG = {\n",
    "    'model_name': 'microsoft/codebert-base',\n",
    "    'max_length': 512,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'dropout': 0.1,\n",
    "    'max_grad_norm': 1.0,\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Dataset =============\n",
    "train_df = pd.read_parquet('task_b_train.parquet')\n",
    "val_df = pd.read_parquet('task_b_val.parquet')\n",
    "print(f\"üìä Train: {len(train_df):,}, Val: {len(val_df):,}\")\n",
    "\n",
    "# quick sanity: show label distribution on validation\n",
    "print(val_df['label'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac404a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== TF-IDF Baseline (Task B) =============\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "OUT_DIR = 'outputs/002-zeev-mgc-detection'\n",
    "os.makedirs(os.path.join(OUT_DIR, 'results'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, 'models'), exist_ok=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3), analyzer='word', token_pattern=r\"\\b\\w+\\b\", min_df=2)\n",
    "X_train = vectorizer.fit_transform(train_df['code'])\n",
    "X_val = vectorizer.transform(val_df['code'])\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "acc = accuracy_score(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(f\"TF-IDF Macro F1: {macro_f1:.4f}\")\n",
    "print(report)\n",
    "\n",
    "# save artifacts\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "joblib.dump(vectorizer, os.path.join(OUT_DIR, 'models', f'tfidf_vectorizer_{ts}.pkl'))\n",
    "joblib.dump(clf, os.path.join(OUT_DIR, 'models', f'tfidf_clf_{ts}.pkl'))\n",
    "\n",
    "# write results markdown (Task B template)\n",
    "md_path = os.path.join(OUT_DIR, 'results', f'tfidf_task_b_{ts}.md')\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write('# TF-IDF Baseline Results (Task B)\\n\\n')\n",
    "    f.write(f'**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n')\n",
    "    f.write('## Metrics\\n')\n",
    "    f.write('| Metric | Value |\\n')\n",
    "    f.write('|--------|-------|\\n')\n",
    "    f.write(f\n",
    ")\n",
    "    f.write(f\n",
    ")\n",
    "    f.write('## Configuration\\n')\n",
    "    f.write(\n",
    "    f.write(\n",
    "3\n",
    ")\n",
    "    f.write(\n",
    "    f.write(\n",
    "    f.write('## Detailed Report (validation)\\n')\n",
    "    f.write('```\\n')\n",
    "    f.write(report)\n",
    "    f.write('```\\n')\n",
    "\n",
    "print('Wrote TF-IDF results to', md_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a1139",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: CodeBERT training\n",
    "\n",
    "Run the following cells to train CodeBERT on the uploaded Task B data (GPU recommended). To avoid downloading model weights, run the trainer locally with `--allow_model_download` flagged in the repo script; otherwise Colab will download weights automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Model =============\n",
    "class CodeBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=11, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.codebert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.codebert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(self.dropout(cls_output))\n",
    "\n",
    "print(\"ü§ñ Loading CodeBERT...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "model = CodeBERTClassifier(CONFIG['model_name'], dropout=CONFIG['dropout']).to(device)\n",
    "print(f\"‚úÖ Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ad1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== DataLoaders =============\n",
    "train_dataset = CodeDataset(train_df, tokenizer, CONFIG['max_length'])\n",
    "val_dataset = CodeDataset(val_df, tokenizer, CONFIG['max_length'])\n",
    "\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "# Verify batch shape\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"‚úÖ Batch shape: {sample['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Training Functions =============\n",
    "def train_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "        loss = criterion(logits, batch['label'].to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, labels, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            probs.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            labels.extend(batch['label'].numpy())\n",
    "    return f1_score(labels, preds, average='macro'), classification_report(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== TRAINING =============\n",
    "print(\"üöÄ Training on RAW code (Phase 3)...\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    ",\n",
    "\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "   Loss: {loss:.4f}\\\n",
    ",\n",
    ",\n",
    ",\n",
    "   Val F1: {f1:.4f}\\\n",
    ",\n",
    ","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
