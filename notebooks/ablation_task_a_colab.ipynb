{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Phase 5: Ablation Study - Preprocessed Code (Task A)\n",
        "\n",
        "**Project**: SemEval-2026 Task 13 - Machine-Generated Code Detection  \n",
        "**Phase**: 5 (T029-T035) - Ablation Study  \n",
        "**Input**: PREPROCESSED code (Comments removed, whitespace normalized)  \n",
        "**Hypothesis**: Performance should be LOWER than raw code (proving \"Fingerprint Paradox\")  \n",
        "\n",
        "## Setup\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**\n",
        "2. Upload `task_a_train.parquet` and `task_a_val.parquet`\n",
        "3. Run all cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print(\"üìÅ Upload task_a_train.parquet and task_a_val.parquet\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ============== Configuration ==============\n",
        "SEED = 42\n",
        "CONFIG = {\n",
        "    'model_name': 'microsoft/codebert-base',\n",
        "    'max_length': 512,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 3,\n",
        "    'learning_rate': 2e-5,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'dropout': 0.1,\n",
        "    'max_grad_norm': 1.0,\n",
        "}\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Dataset (Preprocessed) ==============\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        # ‚ö†Ô∏è CRITICAL CHANGE FOR ABLATION: Use 'code_preprocessed' column\n",
        "        self.codes = df['code_preprocessed'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.codes)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.codes[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_parquet('task_a_train.parquet')\n",
        "val_df = pd.read_parquet('task_a_val.parquet')\n",
        "print(f\"üìä Train: {len(train_df):,}, Val: {len(val_df):,}\")\n",
        "print(f\"‚ö†Ô∏è Using PREPROCESSED code column for training\")\n",
        "print(f\"Sample: {train_df['code_preprocessed'].iloc[0][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Model ==============\n",
        "class CodeBERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.codebert = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.codebert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0]\n",
        "        return self.classifier(self.dropout(cls_output))\n",
        "\n",
        "print(\"ü§ñ Loading CodeBERT...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "model = CodeBERTClassifier(CONFIG['model_name'], dropout=CONFIG['dropout']).to(device)\n",
        "print(f\"‚úÖ Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== DataLoaders ==============\n",
        "train_dataset = CodeDataset(train_df, tokenizer, CONFIG['max_length'])\n",
        "val_dataset = CodeDataset(val_df, tokenizer, CONFIG['max_length'])\n",
        "\n",
        "g = torch.Generator().manual_seed(SEED)\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, generator=g)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(f\"‚úÖ Batch shape: {sample['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Training Setup ==============\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "total_steps = len(train_loader) * CONFIG['epochs']\n",
        "warmup_steps = int(CONFIG['warmup_ratio'] * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "print(f\"üìà Steps: {total_steps}, Warmup: {warmup_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Training Functions ==============\n",
        "def train_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "        loss = criterion(logits, batch['label'].to(device))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    preds, labels, probs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            labels.extend(batch['label'].numpy())\n",
        "    return f1_score(labels, preds), roc_auc_score(labels, probs), classification_report(labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== TRAINING (Ablation) ==============\n",
        "print(\"üöÄ Training on PREPROCESSED code (Phase 5 Ablation)...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "epoch_losses = []\n",
        "best_f1, best_state = 0.0, None\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    print(f\"\\nüìç Epoch {epoch + 1}/{CONFIG['epochs']}\")\n",
        "    loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
        "    epoch_losses.append(loss)\n",
        "    print(f\"   Loss: {loss:.4f}\")\n",
        "    \n",
        "    f1, roc_auc, _ = evaluate(model, val_loader, device)\n",
        "    print(f\"   Val F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "    \n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        print(f\"   ‚≠ê New best!\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Final Evaluation ==============\n",
        "model.load_state_dict(best_state)\n",
        "model.to(device)\n",
        "f1, roc_auc, report = evaluate(model, val_loader, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä ABLATION RESULTS (PREPROCESSED CODE)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üéØ F1: {f1:.4f}\")\n",
        "print(f\"üìà ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Save & Download ==============\n",
        "torch.save(best_state, 'model_task_a_preprocessed.pt')\n",
        "print(\"üíæ Saved: model_task_a_preprocessed.pt\")\n",
        "\n",
        "# Results markdown\n",
        "results = f\"\"\"# CodeBERT Results - PREPROCESSED Code (Task A Ablation)\n",
        "\n",
        "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (Colab T4 GPU)\n",
        "\n",
        "## Metrics\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **F1** | **{f1:.4f}** |\n",
        "| ROC-AUC | {roc_auc:.4f} |\n",
        "\n",
        "## Training Losses\n",
        "\"\"\" + \"\\n\".join([f\"- Epoch {i+1}: {l:.4f}\" for i, l in enumerate(epoch_losses)]) + f\"\"\"\n",
        "\n",
        "## Classification Report\n",
        "```\n",
        "{report}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "with open('codebert_preprocessed_task_a.md', 'w') as f:\n",
        "    f.write(results)\n",
        "\n",
        "# Download both files\n",
        "files.download('model_task_a_preprocessed.pt')\n",
        "files.download('codebert_preprocessed_task_a.md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== Per-Language F1 (T037-T040) ==============\n",
        "def compute_per_language_f1(df, y_pred):\n",
        "    \"\"\"Compute F1 per language.\"\"\"\n",
        "    results = []\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üìä PER-LANGUAGE ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"{'Language':<15} {'Samples':<10} {'F1 Score':<10}\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    for lang in sorted(df['language'].unique()):\n",
        "        mask = df['language'] == lang\n",
        "        lang_labels = df.loc[mask, 'label'].values\n",
        "        lang_preds = y_pred[mask]\n",
        "        f1 = f1_score(lang_labels, lang_preds)\n",
        "        results.append({'language': lang, 'samples': mask.sum(), 'f1': f1})\n",
        "        print(f\"{lang:<15} {mask.sum():<10} {f1:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Get predictions on validation set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Getting predictions\"):\n",
        "        logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Compute per-language F1\n",
        "per_lang = compute_per_language_f1(val_df, all_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üì• Output Handling\n",
        "\n",
        "1. Compare `codebert_preprocessed_task_a.md` with your RAW code results.\n",
        "2. If F1 is LOWER here, you have successfully verified the **Fingerprint Paradox**.\n",
        "3. Add the numbers to your `tasks.md` ablation table."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
