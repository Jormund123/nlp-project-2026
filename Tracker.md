## Progress Tracker

### Day 1
- [X] Step 1: Download dataset
- [X] Step 2: Task A subset
- [X] Step 3: Task B subset
- [ ] Step 4: Preprocessing function
- [ ] Step 5: Tokenizer + data loader
- [ ] Step 6: Task A training script
- [ ] Step 7: Task B training script
- [ ] Step 8: Train Task A
- [ ] Step 9: Train Task B
- [ ] Step 10: Evaluate both

### Day 2
- [ ] Step 11: HP sweep Task A
- [ ] Step 12: HP sweep Task B + Focal Loss
- [ ] Step 13: Final Task A model
- [ ] Step 14: Final Task B model
- [ ] Step 15: Confusion matrix A
- [ ] Step 16: Confusion matrix B
- [ ] Step 17: Loss curves
- [ ] Step 18: Per-language F1
- [ ] Step 19: Per-generator F1
- [ ] Step 20: Top 3 confusion pairs
- [ ] Step 21: Kaggle submissions
- [ ] Step 22: Submit to Kaggle

### Day 3
- [ ] Step 23: LaTeX template
- [ ] Step 24: Introduction
- [ ] Step 25: Methodology
- [ ] Step 26: Insert figures
- [ ] Step 27: Results section
- [ ] Step 28: Analysis
- [ ] Step 29: Conclusion
- [ ] Step 30: References
- [ ] Step 31: Final review
- [ ] Step 32: Export PDF

---

## Results (Fill in as you go)

### Task A (Binary Classification)
| Metric | Value |
|--------|-------|
| Validation F1 | ___ |
| Kaggle Score | ___ |
| Best Learning Rate | ___ |

### Task B (11-class)
| Metric | Value |
|--------|-------|
| Validation Macro F1 | ___ |
| Kaggle Score | ___ |
| Best Learning Rate | ___ |

### Per-Language F1 (Task A)
| Language | F1 |
|----------|-----|
| Python | ___ |
| Java | ___ |
| C++ | ___ |

### Per-Generator F1 (Task B)
| Generator | F1 |
|-----------|-----|
| Human | ___ |
| DeepSeek-AI | ___ |
| Qwen | ___ |
| 01-ai | ___ |
| BigCode | ___ |
| Gemma | ___ |
| Phi | ___ |
| Meta-LLaMA | ___ |
| IBM-Granite | ___ |
| Mistral | ___ |
| OpenAI | ___ |